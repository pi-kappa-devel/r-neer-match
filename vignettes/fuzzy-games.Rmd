---
title: "Fuzzy Games Matching"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fuzzy Games Matching}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

Load the library and set the seed for reproducibility.
```{r setup}
library(neermatch)

seed <- 222L
set.seed(seed)
```

# Data Description
## Left Dataset
For this example, we use a subset of the dataset `game_reviews` shipped with `rmlsem`. The subset is created by selecting all the records in `game_reviews` having titles starting with either `"Metal Slug"` or `"Metal Gear"`. The selection results in 36 records. To illustrate how `rmlsem` makes use of information found in multiple fields, the records are selected so that many of them have the exact same title, developer, and release year and differ only in the release platform. The `right` dataset is constructed by creating a random noisy version of the left dataset. 

## Right Dataset
The `right` dataset is created by copying the `left` and introducing noise in the `title` and `developer` columns. Three characters are randomly removed from the `title` column, and one character is randomly removed from the `developer` column. The mutated `developer` column is stored in the column `dev` of the `right` dataset.

## Matches
The matching examples are passed to the matching model as indices. The `rmlsem` package can be used to link datasets with records/entities having many-to-many relations. This example illustrates this point by randomly creating 3 duplicate matching records on the `right` dataset.  For this application, the matches are constructed by the rows of the `left` and `right` datasets that have the same index and the 3 duplicate matches we created. 

The function `fuzzy_games_example_data` automates the construction of the left, right, and matches datasets from the `game_reviews` dataset. For more details on the data, see the `fuzzy_games_example_data` function and the `game_reviews` dataset documentation. 

```{r load-data}
matching_data <- fuzzy_games_example_data()
left <- matching_data$left
right <- matching_data$right
matches <- matching_data$matches
```

# Matching Model Setup
A matching model initialization requires instructions on constructing the similarity map between the `left` and `right` datasets. The instructions are passed to the model as a named list that specifies

1. which fields are used from each dataset and how they are associated with each other, and
3. how to encode them, i.e., how to calculate the similarity values.

If the column names of the `left` and `right` are the same, it suffices to supply the common name to the similarity map instruction list (see, e.g., the `title`, `platform`, `year`, and `reviews` items below). If two columns have different names, the instruction should have the form `left_name~right_name`, as it is, for instance, the case for the developing studio in this application (see item `developer~dev`). Not all columns of the datasets need to be used in the instructions. In this application, the column `score` is not used.

The model can be instructed to calculate one or more similarities for each column association. For instance, the instructions of this example specify two similarity calculations for the `platform` and `year` associations and one for the `title`, `developer~dev`, and `reviews`. The `rmlsem` provides a set of predefined similarity functions that are calculated directly in native code, and if the system where the model is fit supports it, in parallel. The predefined similarities are `discrete`, `euclidean`, `gaussian`, `levenshtein`, `jaro`, and `jaro_winkel`. In addition, the ratios `partial`, `token_sort`, `partial_token_sort`, `token_set`, and  `partial_token_set` are supported. The `discrete` similarity can be applied to both strings and numerical fields, the `euclidean` and `gaussian` only to numerical fields. The  `levenshtein`, `jaro`, `jaro_winkel`, `partial`, `token_sort`, `partial_token_sort`, `token_set`, and  `partial_token_set` functions are compatible only with string fields. The string similarities and ratios are calculated using the native implementation of [RapidFuzz](https://maxbachmann.github.io/RapidFuzz/).

The most efficient way to fit the model is to use the predefined similarities provided by `rmlsem`. If further customization is required, and this cannot be achieved via data preprocessing, one can also pass a custom similarity function in the instructions. For instance, the association `year` in this example uses the custom similarity function `my_euclidean`.
```{r prepare-similarity-map}
similarity_instructions <- list(
  title = list("jaro_winkler"),
  platform = list("levenshtein", "discrete"),
  year = list("euclidean", "discrete"),
  `developer~dev` = list("jaro"),
  reviews = list("euclidean")
)

similarity_map <- SimilarityMap(similarity_instructions)
show(similarity_map)
```

A `matching_model` object is constructed by passing the similarity map instructions. The model prepares native bindings based on the passed instructions and uses them whenever the model is fit or evaluated.
```{r make-dl-matching-model}
model <- DLMatchingModel(similarity_map)

## model |>
##   plot(show_shapes = TRUE, show_layer_names = TRUE, show_dtype = TRUE)
```

The model is compiled in the usual way. The compile function wraps the `keras::compile` functions, so all the functionality and options in the latter can be used here.
```{r compile-dl-model}
model |>
  compile(
    loss = tensorflow::tf$keras$losses$BinaryCrossentropy(),
    optimizer = tensorflow::tf$keras$optimizers$Adam(learning_rate = 0.001)
  )
```

# Matching Model Fit and Evaluation
The model is fit using the `fit` function. The `fit` function extends the functionality of `keras::fit` to accommodate the `matching_model` functionality. Firstly, instead of passing features and label arguments, `fit` expects a `SimilarityEncoder` object. The `SimilarityEncoder` object can be acquired by calling the `encode` function with the similarity map instructions and the `left`, `right`, and `matches` datasets. The `encode` function calculates the similarity values for each pair of records in the `left` and `right` datasets based on the similarity map instructions. In addition, the `encode` automatically selects counterexamples for each match in the `matches` dataset. The counterexamples are selected based on the `mismatch_share` parameter. The `mismatch_share` parameter controls the ratio of counterexamples to matches. For instance, if `mismatch_share = 0.5`, the `encode` function selects 50% of the possible counterexamples for each match. The counterexamples are selected randomly from the `right` dataset. The `seed` parameter is used to control the randomness of the counterexample selection.

For each matched row index $l$ of the `left` dataset, the `matching_model` can calculate up to `nrow(right)`$-\ m_{l}$ non-matching examples, where $m_{r}$ is the number of matches with $l$ as the left index. Including all the non-matching examples can lead to a highly unbalanced matching dataset. For instance, this example uses `r nrow(matches)`, which allows us to construct `r nrow(matches)*(nrow(matches) - 1)` non-matching examples for every non-duplicated match. In this application, we use `mismatch_share = 0.2`, which means that for each matching example provided in `matches` we use `floor(mismatch_share*(nrow(matches) - 1))`=`r floor(0.2*(nrow(matches) - 1))` non-matching examples.

The remaining arguments are similar to `keras::fit`. The callback `LogModN` is exported by the `rmlsem` and reports the progress of the `fit` call in a compact table format every $n=500$ epochs. 
```{r fit-dl-model}
## fitted <- fit(
##   model,
##   left, right, matches,
##   epochs = 10L, batch_size = 32L, verbose = 0L
## )
```

The `rmlsem::evaluate` function overloads the `keras::evaluate` function. Similar to fitting a `matching_model`, the `evaluate` call expects a `SimilarityEncoder` object. Any additional keyword arguments given in `evaluate` are passed down to `keras::evaluate`.
```{r evaluate-dl-model}
## evaluate(model, left, right, matches, verbose = 0L)
```

# Predictions and Suggestions
Matching predictions can be obtained in two ways from the fitted model. Either by calling `predict` or by calling `suggest`. The `predict` function returns a vector of prediction probabilities for each combination of `left` and `right` records. The prediction probabilities are stored in row-major order. First, the matching probabilities of the first row of `left` with the rows 1 to `nrow(right)` of `right` are given. Then, the probabilities of the second row of `left` with rows 1 to `nrow(right)` of right, and so on, so forth. In total, the `predict` function returns a vector with `nrow(left)*nrow(right)` elements if a mismatch share of 1.0 is used.
```{r dl-model-predictions, fig.alt="CDF of DL Matching Probabilities"}
## model |>
##   predict(left, right, verbose = 0) |>
##   ecdf() |>
##   plot(main = "CDF of DL Matching Probabilities", xlab = "Matching Probability")
```

The `suggest` function returns the best matching predictions of the model for each row of the `left` dataset. The prediction probabilities of `predict` are grouped by the indices of the `left` dataset and sorted in descending order. The caller can choose the number of returned suggestions by setting the `count` argument of `suggest`.
```{r dl-model-suggestions}
## suggestions <- model |>
##   suggest(left, right, count = 3L, verbose = 0L)
## suggestions["true_match"] <- apply(suggestions[, c(1, 2)], 1, function(r) {
##   any(r[1] == matches[, 1] & r[2] == matches[, 2])
## })
## duplicates <- matches[(nrow(matches) - 2):nrow(matches), "left"]
## print(duplicates)
## rmarkdown::paged_table(suggestions, options = list(rows.print = 12))
```

```{r dl-local-interpretable-model-agnostic-explanations}
## left_x <- left[matches[37, 1], ]
## right_x <- right[matches[37, 2], ]
## eaten <- sapply(seq(0, 2), function(cf) {
##   surrogate <- lime(model, left_x, right_x, complexity_factor = cf)
##   summary(surrogate)
## })
```

# Logic Tensor Model

```{r make-ltn-matching-model}
## model <- NSMatchingModel(similarity_map)

## model |>
##   plot(show_shapes = TRUE, show_layer_names = TRUE, show_dtype = TRUE)
```

```{r compile-ltn-model}
## model |>
##   compile(
##     optimizer = tensorflow::tf$keras$optimizers$Adam(learning_rate = 0.001)
##   )
```

```{r fit-ltn-model}
## fit(
##   model, left, right, matches,
##   epochs = 10L, batch_size = 16L, verbose = 1L, log_mod_n = 50L
## )
```

```{r evaluate-ltn-model}
# evaluate(model, left, right, matches, batch_size = 200L)
```

```{r ltn-model-predictions, fig.alt="CDF of NS Matching Probabilities"}
## model |>
##   predict(left, right) |>
##   as.list() |>
##   ecdf() |>
##   plot(
##     main = "CDF of NS Matching Probabilities",
##     xlab = "Matching Probability"
##   )
```

```{r ltn-model-suggestions}
## suggestions <- model |>
##   suggest(left, right, count = 3L)
## suggestions["true_match"] <- apply(suggestions[, c(1, 2)], 1, function(r) {
##   any(r[1] == matches[, 1] & r[2] == matches[, 2])
## })
## print(duplicates)
## rmarkdown::paged_table(suggestions, options = list(rows.print = 12))
```
